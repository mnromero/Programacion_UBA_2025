{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObeIwb_npgeZ"
   },
   "source": [
    "# Big Data y Machine Learning (UBA) 2025\n",
    "## Clase 3 - Parte 2 - Kernels\n",
    "\n",
    "**Objetivo:**\n",
    "Que se familiaricen con el segundo m√©todo no param√©trico - Kernels - para la estimaci√≥n de la distribuci√≥n de densidad de una variable aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√©todos no param√©tricos\n",
    "El objetivo es predecir distribuci√≥n de una variable de inter√©s \n",
    "- ùëå variable aleatoria de inter√©s\n",
    "- ùëì(ùëå) distribuci√≥n de densidad ùëå\n",
    "\n",
    "##### M√©todos\n",
    "- Breve repaso de Histogramas\n",
    "- Kernels con Sckit-learn\n",
    "    -  Tipo de funciones de kernels\n",
    "    -  Opciones de Kernels: Ancho de banda $h$\n",
    "    -  Simulaci√≥n de datos: Sesgo de la estimaci√≥n no parametrica de Kernels\n",
    "- Kernels con Seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos paquetes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm #para crear datos de distribucion normal con otro modulo\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breve repaso de Histogramas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimamos la distribucion de densidad $f(Y)$ una variable aleatoria Y, con la siguiente aproximaci√≥n no parametrica:\n",
    "\n",
    "$$\n",
    "\\hat{f}(y) = \\frac{M}{n} ‚àë^ùëõ_i I(ùëå_ùëñ \\in B_l)  \n",
    "$$\n",
    "Con $B_l$ barra (bin) $l$-√©simo\n",
    "\n",
    "Podemos usar el atributo `hist` de Matplotlib. Ver documentaci√≥n [ac√°](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos\n",
    "np.random.seed(20)\n",
    "X = np.concatenate([np.random.normal(0,1,500), np.random.normal(5,1,500)]).reshape(-1,1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df=pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(X, bins=30, alpha=0.5, color='blue', label='Histograma')\n",
    "plt.xlabel('Valores')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Agregamos l√≠nea vertical con la media\n",
    "mean_value = np.mean(X)\n",
    "plt.axvline(mean_value, color='red', linestyle='dashed', linewidth=1, label='Media')\n",
    "plt.legend()  # Show legend with label for the mean line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel:\n",
    "A cada observaci√≥n le estima una peque√±a funci√≥n de densidad y suma todas las peque√±as funciones\n",
    "\n",
    "$$\n",
    "ùëì(ùë¶_0)= \\frac{1}{n} ‚àë^ùëõ_i  \\frac{1}{h} ùêæ(\\frac{ùëå_ùëñ‚àíùë¶_0}{h}) \n",
    "$$\n",
    "\n",
    "- $K(z)$  funci√≥n Kernel continua (y generalmente) sim√©trica \n",
    "- $h$ ancho de banda (smoothing bandwidth) --> Controla qu√© tan ‚Äúsuave‚Äù es la densidad \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar el [m√≥dulo neighbors de Scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estimar una densidad usando kernels tenemos la siguiente funci√≥n: \n",
    "\n",
    "<code> sklearn.neighbors.KernelDensity(*, bandwidth=1.0, algorithm='auto', kernel='gaussian', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)</code>\n",
    "\n",
    "donde algunos par√°metros importantes son:\n",
    "- <code>bandwidth</code> (valor por default: 1.0)\n",
    "- <code>kernel</code> (valor por default: 'gaussian')\n",
    "\n",
    "Scikit learn nos permite cambiar el kernel y probar varios y cu√°l ajusta mejor a los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(X, bins=30, density=True, alpha=0.3, color='blue', label='Histograma') # mantenemos el histogramas para comparar\n",
    "\n",
    "# Rango de valores para eje x (para graficar la funcion de Kernel)\n",
    "X_plot = np.linspace(min(X), max(X), 1000).reshape(-1,1)\n",
    "\n",
    "# Estimamos la funcion de Kernel Gaussiana y todas sus opciones de default\n",
    "kde = KernelDensity().fit(X)\n",
    "    \n",
    "# Usar la KDE para estimar la densidad para cada valor de X\n",
    "log_densities = kde.score_samples(X_plot)\n",
    "densities = np.exp(log_densities)\n",
    "    \n",
    "# Grafico de kernel\n",
    "plt.plot(X_plot[:,0], densities, color='red', label=f'Gaussian Kernel')\n",
    "\n",
    "# Agregamos l√≠nea vertical con la media\n",
    "mean_value = np.mean(X)\n",
    "plt.axvline(mean_value, color='green', linestyle='dashed', linewidth=1, label='Media')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Estimaci√≥n con Kernel Gaussiano')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipos de kernels (disponibles en Scikit learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernels\n",
    "kernels = [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"] \n",
    "  \n",
    "# Figura con 3 filas y 2 columnas\n",
    "fig, ax = plt.subplots(3, 2) \n",
    "# Tama√±o de la figura\n",
    "fig.set_figheight(15) \n",
    "fig.set_figwidth(10)   \n",
    "# T√≠tulo \n",
    "fig.suptitle(\"Tipos de kernels\") \n",
    "\n",
    "# 1D array de valores de x para graficar la distribuci√≥n \n",
    "x_plot = np.linspace(-6, 6, 1000) # 1000 valores de -6 a 6 separados con la misma distancia entre s√≠\n",
    "x_plot = x_plot.reshape(-1,1) # formato 2D array (necesario para scikit learn)\n",
    "x_orig = np.zeros((1, 1)) # punto (0,0)\n",
    "  \n",
    "# Graficamos usando los distintos kernels \n",
    "for i, kernel in enumerate(kernels): \n",
    "    # Ajustamos el modelo \n",
    "    kde = KernelDensity(kernel=kernel).fit(x_orig) # usamos el punto (0,0)\n",
    "    # log de la densidad de probabilidad (PDF)\n",
    "    log_dens = kde.score_samples(x_plot) \n",
    "      \n",
    "    # Distribuciones \n",
    "    ax[i // 2, i % 2].fill(x_plot[:, 0], np.exp(log_dens)) \n",
    "    # i//2 nos permite referirnos a la fila del subplot, e i%2 nos permite referirnos a la columna\n",
    "    # T√≠tulo y labels de los subplots \n",
    "    ax[i // 2, i % 2].set_title(kernel.capitalize()) \n",
    "    ax[i // 2, i % 2].set_xlim(-3, 3) \n",
    "    ax[i // 2, i % 2].set_ylim(0, 1) \n",
    "    ax[i // 2, i % 2].set_ylabel(\"Densidad\") \n",
    "    ax[i // 2, i % 2].set_xlabel(\"x\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma forma, en un gr√°fico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernels\n",
    "kernels = [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"] \n",
    "  \n",
    "# Grafico\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for k in kernels:\n",
    "    # Ajustamos el modelo \n",
    "    kde = KernelDensity(kernel=k).fit(x_orig) # usamos el punto (0,0)\n",
    "    # log de la densidad de probabilidad (PDF)\n",
    "    log_dens = kde.score_samples(x_plot) \n",
    "    \n",
    "    # Graficar la estimacion para cada kernel\n",
    "    plt.plot(x_plot[:,0], np.exp(log_dens), label=f'{k.capitalize()} Kernel')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Estimaci√≥n con diferentes Kernels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuamos con el ejemplo de la variable X creada (en la clase pasada) probando los distintos tipos de kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de kernels a probar\n",
    "kernels = [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"] \n",
    "\n",
    "# Grafico\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(X, bins=30, density=True, alpha=0.3, color='blue', label='Histograma')\n",
    "\n",
    "for k in kernels:\n",
    "    kde = KernelDensity(kernel=k).fit(X)\n",
    "    \n",
    "    # Usar la KDE para estimar la densidad para cada valor de X\n",
    "    log_densities = kde.score_samples(X_plot)\n",
    "    densities = np.exp(log_densities)\n",
    "    \n",
    "    # Graficar para cada kernel\n",
    "    plt.plot(X_plot[:,0], densities, label=f'{k.capitalize()} Kernel')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Estimaci√≥n con diferentes Kernels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opciones de Kernels: Ancho de banda $h$\n",
    "Ahora veamos qu√© ocurre si para un mismo kernel, cambiamos los **anchos de banda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchos de banda\n",
    "bandwidths = [0.5, 0.75, 1, 1.25, 1.5, 1.75] \n",
    "  \n",
    "# Figura con 3 filas y 2 columnas\n",
    "fig, ax = plt.subplots(3, 2) \n",
    "# Tama√±o de la figura\n",
    "fig.set_figheight(15) \n",
    "fig.set_figwidth(10)   \n",
    "# T√≠tulo \n",
    "fig.suptitle('Kernel Gaussiano, con distintos anchos de banda')\n",
    "\n",
    "# Graficamos usando los distintos kernels \n",
    "for i, bw in enumerate(bandwidths): \n",
    "    # Ajustamos el modelo \n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bw).fit(x_orig) # usamos el punto (0,0)\n",
    "    # log de la densidad de probabilidad (PDF)\n",
    "    log_dens = kde.score_samples(x_plot) \n",
    "      \n",
    "    # Distribuciones \n",
    "    ax[i // 2, i % 2].fill(x_plot[:, 0], np.exp(log_dens)) \n",
    "    # i//2 nos permite referirnos a la fila del subplot, e i%2 nos permite referirnos a la columna\n",
    "    # T√≠tulo y labels de los subplots \n",
    "    ax[i // 2, i % 2].set_title('Kernel Gaussiano con bandwidth='+str(bw)) \n",
    "    ax[i // 2, i % 2].set_xlim(-3, 3) \n",
    "    ax[i // 2, i % 2].set_ylim(0, 1) \n",
    "    ax[i // 2, i % 2].set_ylabel('Densidad') \n",
    "    ax[i // 2, i % 2].set_xlabel('x') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchos de banda\n",
    "bandwidths = [0.5, 0.75, 1, 1.25, 1.5, 1.75] \n",
    "  \n",
    "# Grafico\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for bw in bandwidths:\n",
    "    # Ajustamos el modelo \n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bw).fit(x_orig) # usamos el punto (0,0)\n",
    "    # log de la densidad de probabilidad (PDF)\n",
    "    log_dens = kde.score_samples(x_plot) \n",
    "    \n",
    "    # Graficar la estimacion para cada kernel\n",
    "    plt.plot(x_plot[:,0], np.exp(log_dens), label='Kernel Gaussiano con bandwidth='+str(bw))\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Kernel Gaussiano, con distintos anchos de banda') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchos de banda\n",
    "bandwidths = [0.5, 0.75, 1, 1.25, 1.5, 1.75] \n",
    "\n",
    "# Grafico\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(X, bins=30, density=True, alpha=0.5, color='blue', label='Histograma')\n",
    "\n",
    "for bw in bandwidths:\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bw).fit(X)\n",
    "    \n",
    "    # Usar la KDE para estimar la densidad para cada valor de X\n",
    "    log_densities = kde.score_samples(X_plot)\n",
    "    densities = np.exp(log_densities)\n",
    "    \n",
    "    # Graficar para cada kernel\n",
    "    plt.plot(X_plot[:,0], densities, label='Bandwidth='+str(bw))\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Estimaci√≥n de Kernel Gaussiano con diferentes ancho de banda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulaci√≥n de datos: Sesgo de la estimaci√≥n no parametrica de Kernels\n",
    "Ahora veamos un ejemplo donde creamos datos ficticios, esto implica que conocemos la verdadera forma en la que se generan los datos, para comparar la estimaci√≥n no param√©trica de Kernels y su aproximaci√≥n a la verdadera funci√≥n de densidad. Se puede demostrar formalmente, que la estimaci√≥n no param√©trica de Kernels (e histograma) es *sesgada*. Por lo que, aqu√≠ estamos ilustrando ese concepto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una distribuci√≥n\n",
    "n = 10000000\n",
    "np.random.seed(100)\n",
    "X = np.concatenate((np.random.normal(0, 1, int(0.6 * n)), np.random.normal(10, 1, int(0.4 * n)))) \n",
    "# Creamos X concatenando datos de dos distribuciones normales\n",
    "# primero 60 datos de una distribuci√≥n normal con media 0 y desv√≠o 1\n",
    "# luego, 40 datos de una normal con media 10 y desv√≠o 1\n",
    "X = X.reshape(-1,1)\n",
    "\n",
    "X_plot = np.linspace(-5, 15, 1000).reshape(-1,1)\n",
    "# Usaremos X para estimar la densidad y calcularemos la densidad para los puntos de X_plot \n",
    "\n",
    "# Calcular la \"verdera\" densidad para los puntos X_plot\n",
    "true_density = 0.6 * norm(0, 1).pdf(X_plot[:, 0]) + 0.4 * norm(10, 1).pdf(X_plot[:, 0]) \n",
    "  \n",
    "# Gr√°fico\n",
    "fig, ax = plt.subplots() \n",
    "  \n",
    "# Gr√°fico de la verdadera densidad \n",
    "ax.fill( \n",
    "    X_plot[:, 0], true_density,  \n",
    "    fc='black', alpha=0.2,  \n",
    "    label='Verdadera Distribuci√≥n'\n",
    ") \n",
    "  \n",
    "# Estimar la densidad de X usando kernel gaussiano y bandwidth de 0.5 \n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X) \n",
    "# Log de la PDF \n",
    "log_dens = kde.score_samples(X_plot) \n",
    "  \n",
    "# Densidad \n",
    "ax.plot( \n",
    "    X_plot[:, 0], np.exp(log_dens), \n",
    "    color='blue', \n",
    "    linestyle='-', \n",
    "    label='Densidad con kernel Gaussiano'\n",
    ")  \n",
    "ax.set_xlim(-4, 15) \n",
    "ax.set_ylim(0, 0.3) \n",
    "#ax.grid(True) \n",
    "ax.legend(loc='upper right')\n",
    "plt.title('Sesgo de la estimaci√≥n por Kernel') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para elegir el bandwidth con cross-validation (CV) que explicaremos en mayor detalle en la clase 8 tutorial 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tarea para la casa: Mostrar el sesgo del histograma y esta funcion verdadera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels con Seaborn\n",
    "Continuaremos con el ejemplo utilizando la base de datos de propinas del modulo de `seaborn`. Para m√°s informaci√≥n ver [seaborn](https://seaborn.pydata.org/)\n",
    "La funci√≥n de seaborn para graficar Kernels es [kdeplot()](https://seaborn.pydata.org/generated/seaborn.kdeplot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la base de datos de propinas\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veamos la estadistica descriptiva por grupo\n",
    "tips.groupby('sex').describe().round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimamos y graficamos la funci√≥n por Kernels\n",
    "sns.kdeplot(data=tips, x='tip') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente podemos mejorar el gr√°fico de seaborn usando las opciones de matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=tips, x='tip')  # funcion de kernel de Seaborn\n",
    "mean_tips = np.mean(tips['tip'])\n",
    "plt.axvline(mean_tips, color='red', linestyle='dashed', linewidth=1, label='Tips promedio')\n",
    "plt.title(\"Distribuci√≥n de propinas (en USD)\")\n",
    "plt.xlabel(\"Propinas (en USD)\")\n",
    "plt.legend()  # Nos muestra la leyenda para la media de tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n podemos hacer el grafico original del histograma (ver clase 8) y sumar con la opcion `kde=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=tips['tip'], stat='density', kde=True) # funcion de histograma de Seaborn\n",
    "mean_tips = np.mean(tips['tip'])\n",
    "plt.axvline(mean_tips, color='red', linestyle='dashed', linewidth=1, label='Tips promedio')\n",
    "plt.title(\"Distribuci√≥n de propinas (en USD)\")\n",
    "plt.xlabel(\"Propinas (en USD)\")\n",
    "plt.legend()  # Nos muestra la leyenda para la media de tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, podemos comparar las distribuciones de densidad de kernel entre hombres y mujeres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkeando el promedio de propinas\n",
    "mean_tips_male = tips[tips['sex'] == 'Male']['tip'].mean()\n",
    "print(mean_tips_male.round(2))\n",
    "mean_tips_female = tips[tips['sex'] == 'Female']['tip'].mean()\n",
    "print(mean_tips_female.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=tips, x='tip',hue=\"sex\",  multiple=\"stack\", )  # funcion de kernel de Seaborn\n",
    "\n",
    "plt.axvline(mean_tips_male, color='blue', linestyle='dashed', linewidth=1, label='Male')\n",
    "plt.axvline(mean_tips_female, color='red', linestyle='dashed', linewidth=1, label='Female')\n",
    "\n",
    "plt.title(\"Distribuci√≥n de propinas (en USD)\")\n",
    "plt.xlabel(\"Propinas (en USD)\")\n",
    "plt.legend()  # Nos muestra la leyenda para la media de tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tarea para la casa: jugar con la opcion de ver este grafico en dos paneles y sumarle el valor del promedio de cada grupo"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
